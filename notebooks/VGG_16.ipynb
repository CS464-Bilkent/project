{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KSkGoLiYkdbY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import mlxtend\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "import gc\n",
        "from numba import cuda\n",
        "import shutil\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tpLyYXjkh7o",
        "outputId": "0d79cea9-5adf-498a-d63f-1b9eccd70e4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "def load_images(data_path, target_size=(224, 224), num_images_per_class=100):\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = os.listdir(data_path)\n",
        "    class_names.sort()\n",
        "    if '.DS_Store' in class_names:\n",
        "        class_names.remove('.DS_Store')\n",
        "\n",
        "    for label, class_name in enumerate(class_names):\n",
        "        class_path = os.path.join(data_path, class_name)\n",
        "        image_names = os.listdir(class_path)\n",
        "        if '.DS_Store' in image_names:\n",
        "            image_names.remove('.DS_Store')\n",
        "\n",
        "        # Randomly select images from each class\n",
        "        selected_images = random.sample(image_names, min(len(image_names), num_images_per_class))\n",
        "        for image_name in selected_images:\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "            try:\n",
        "                image = Image.open(image_path).convert('RGB')\n",
        "                image = image.resize(target_size)\n",
        "                images.append(np.array(image))\n",
        "                labels.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {image_path}: {e}\")\n",
        "\n",
        "    return np.array(images), np.array(labels), class_names\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/colab_data/animals\"\n",
        "images, labels, class_names = load_images(data_path)\n"
      ],
      "metadata": {
        "id": "dlu9qkMilziZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KsE3BckmRpQ",
        "outputId": "6723181a-9a40-4f32-c389-9e48e7955e0f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 224, 224, 3)\n",
            "(100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zipped_data = list(zip(labels, images))\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(zipped_data)\n",
        "\n",
        "labels, images = zip(*zipped_data)\n",
        "\n",
        "labels = np.array(labels)\n",
        "images = np.array(images)\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl_5FgR2mMGy",
        "outputId": "35b86723-e7ba-47e3-fb5e-7688f90a4dec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 224, 224, 3)\n",
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Validation set size: {X_val.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs2YKLOopLJV",
        "outputId": "e7262f3d-c234-49ed-a0cc-c365b40d6ad0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 600\n",
            "Validation set size: 200\n",
            "Test set size: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) #default values for ImageNet dataset, where model is trained\n",
        "])\n",
        "\n",
        "#transform np images to pil images.\n",
        "X_train = torch.stack([transform(img) for img in X_train])\n",
        "X_val = torch.stack([transform(img) for img in X_val])\n",
        "X_test = torch.stack([transform(img) for img in X_test])\n",
        "\n",
        "y_train = torch.tensor(y_train)\n",
        "y_val = torch.tensor(y_val)\n",
        "y_test = torch.tensor(y_test)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Data loaders prepared!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wTYHqKxpxNa",
        "outputId": "ef1bf0db-1c4c-4141-f0be-e2c57c141917"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaders prepared!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.VGG16_Weights.DEFAULT\n",
        "auto_transforms = weights.transforms()\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = torchvision.models.vgg16(weights=weights).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeGKH2Bbpgfo",
        "outputId": "825b3624-7da6-44c6-9a40-3b9c7037b840"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:07<00:00, 76.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bgXhu_YrflM",
        "outputId": "2f412640-8b79-4fd4-ba7f-fc204140d6ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OPTIONAL\n",
        "num_classes = len(class_names)\n",
        "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes) #reshape model output layer to adapt Animals dataset"
      ],
      "metadata": {
        "id": "YuvvWgijsxDG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "IVV9HOQctuhK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the selected device\n",
        "model = model.to(device)\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh6MlJnzttZ0",
        "outputId": "22ed79db-ec1e-4b8e-ec85-6b935a1f949f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, freeze=True):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss_history = []\n",
        "    train_accuracy_history = []\n",
        "\n",
        "    #freeze feature extraction layer params. only fine tunes classification layers.\n",
        "    if freeze:\n",
        "      for param in model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move data to GPU\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Calculate loss\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        train_loss_history.append(epoch_loss)\n",
        "        train_accuracy_history.append(epoch_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
        "\n",
        "    return train_loss_history, train_accuracy_history\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss_history = []\n",
        "    val_accuracy_history = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move data to GPU\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(val_loader)\n",
        "        epoch_accuracy = 100 * correct / total\n",
        "        val_loss_history.append(epoch_loss)\n",
        "        val_accuracy_history.append(epoch_accuracy)\n",
        "\n",
        "        print(f'Validation Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
        "\n",
        "    return val_loss_history, val_accuracy_history\n",
        "\n",
        "\n",
        "def test_model(model, test_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')  # Move data to GPU\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    test_loss = running_loss / len(test_loader)\n",
        "    test_accuracy = 100 * correct / total\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "    return test_loss, test_accuracy, all_predictions, all_labels\n"
      ],
      "metadata": {
        "id": "mpn0l-PjsMLO"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "train_loss, train_accuracy = train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
        "val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n",
        "test_loss, test_accuracy, predictions, true_labels = test_model(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wVYhTNftB1s",
        "outputId": "932e4b36-94f4-4a5a-8473-76f914c42d5b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.4580, Accuracy: 54.00%\n",
            "Epoch 2/10, Loss: 0.3807, Accuracy: 91.33%\n",
            "Epoch 3/10, Loss: 0.1741, Accuracy: 97.17%\n",
            "Epoch 4/10, Loss: 0.1105, Accuracy: 98.17%\n",
            "Epoch 5/10, Loss: 0.0643, Accuracy: 99.50%\n",
            "Epoch 6/10, Loss: 0.0462, Accuracy: 99.50%\n",
            "Epoch 7/10, Loss: 0.0361, Accuracy: 99.83%\n",
            "Epoch 8/10, Loss: 0.0237, Accuracy: 100.00%\n",
            "Epoch 9/10, Loss: 0.0242, Accuracy: 99.83%\n",
            "Epoch 10/10, Loss: 0.0217, Accuracy: 99.83%\n",
            "Validation Loss: 0.1673, Accuracy: 94.50%\n",
            "Test Loss: 0.1660, Accuracy: 94.00%\n"
          ]
        }
      ]
    }
  ]
}